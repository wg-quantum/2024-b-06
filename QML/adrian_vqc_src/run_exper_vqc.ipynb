{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import umap\n",
    "\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit.circuit.library import (\n",
    "    ZZFeatureMap,\n",
    "    ZFeatureMap,\n",
    "    PauliFeatureMap,\n",
    "    RealAmplitudes,\n",
    ")\n",
    "from qiskit_algorithms.optimizers import COBYLA, SPSA\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= bank-additional-full_normalised.csv =========\n",
      "bank-additional-full_normalised\n",
      "(41188, 63)\n",
      "class\n",
      "0    36548\n",
      "1     4640\n",
      "Name: count, dtype: int64\n",
      "========= UNSW_NB15_traintest_backdoor.csv =========\n",
      "UNSW_NB15_traintest_backdoor\n",
      "(95329, 197)\n",
      "class\n",
      "0    93000\n",
      "1     2329\n",
      "Name: count, dtype: int64\n",
      "========= KDD2014_donors_10feat_nomissing_normalised.csv =========\n",
      "KDD2014_donors_10feat_nomissing_normalised\n",
      "(619326, 11)\n",
      "class\n",
      "0    582616\n",
      "1     36710\n",
      "Name: count, dtype: int64\n",
      "========= celeba_baldvsnonbald_normalised.csv =========\n",
      "celeba_baldvsnonbald_normalised\n",
      "(202599, 40)\n",
      "class\n",
      "0    198052\n",
      "1      4547\n",
      "Name: count, dtype: int64\n",
      "========= annthyroid_21feat_normalised.csv =========\n",
      "annthyroid_21feat_normalised\n",
      "(7200, 22)\n",
      "class\n",
      "0    6666\n",
      "1     534\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_paths = [\n",
    "    \"data/bank-additional-full_normalised.csv\",\n",
    "    \"data/census-income-full-mixed-binarized.tar.xz\",\n",
    "    \"data/UNSW_NB15_traintest_backdoor.csv\",\n",
    "    \"data/KDD2014_donors_10feat_nomissing_normalised.csv\",\n",
    "    \"data/celeba_baldvsnonbald_normalised.csv\",\n",
    "    \"data/creditcardfraud_normalised.tar.xz\",\n",
    "    \"data/annthyroid_21feat_normalised.csv\",\n",
    "]\n",
    "\n",
    "data_dict = {}\n",
    "names = []\n",
    "\n",
    "for _path in data_paths:\n",
    "        ext = os.path.splitext(_path)[1][1:]\n",
    "\n",
    "        name = os.path.basename(_path)\n",
    "        names.append(name)\n",
    "        \n",
    "        if ext == \"csv\":\n",
    "            print(\"===\"*3, name, \"===\"*3)\n",
    "\n",
    "            print(name.split(\".\")[0\n",
    "                              ])\n",
    "\n",
    "            df = pd.read_csv(_path)\n",
    "            print(df.shape)\n",
    "            print(df[\"class\"].value_counts())\n",
    "\n",
    "            data_dict[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_RAITO = 3\n",
    "N_SAMPLES = 2_000\n",
    "N_COMPONENTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df):\n",
    "    # Assuming df is your dataframe and 'class' is your column of interest\n",
    "    # Get counts of each class\n",
    "    class_counts = df['class'].value_counts()\n",
    "\n",
    "    # Determine the class labels\n",
    "    minority_class = class_counts.idxmin()\n",
    "    majority_class = class_counts.idxmax()\n",
    "\n",
    "    # Calculate the number of instances to keep for the minority class\n",
    "    minority_class_count = class_counts[minority_class]\n",
    "    majority_class_count = minority_class_count * CLASS_RAITO  # To achieve 1:3 ratio\n",
    "\n",
    "    # Downsample the majority class or upsample the minority class\n",
    "    df_majority_downsampled = resample(\n",
    "        df[df['class'] == majority_class], \n",
    "        replace=False,               # Sample without replacement\n",
    "        n_samples=majority_class_count,  # To match the desired ratio\n",
    "        random_state=42               # For reproducibility\n",
    "    )\n",
    "\n",
    "    # Combine the minority class with the downsampled majority class\n",
    "    df_balanced = pd.concat([df[df['class'] == minority_class], df_majority_downsampled])\n",
    "\n",
    "    # Shuffle the resulting dataframe\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Now, df_balanced has the desired 1:3 class ratio\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "def sample_df(df):\n",
    "    n_ratio = N_SAMPLES / df.shape[0]\n",
    "\n",
    "    sampled_df, _ = train_test_split(df, train_size=n_ratio+0.000002, stratify=df['class'], random_state=42)\n",
    "    return sampled_df\n",
    "\n",
    "\n",
    "def reduce_dimension(df):\n",
    "    features = df.drop(columns=['class'])  \n",
    "\n",
    "    # Initialize UMAP with desired parameters\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=N_COMPONENTS, random_state=42)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    embedding = reducer.fit_transform(features)\n",
    "\n",
    "    X = df.drop(['class'], axis=1)\n",
    "    X_pca = MinMaxScaler().fit_transform(embedding)\n",
    "    y = df['class']\n",
    "\n",
    "    return X, X_pca, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_ml(X_train, y_train, X_test, y_test, y, filename):\n",
    "    # rbf, linear, poly, sigmoidの4つの古典的手法による結果を確認。\n",
    "    classical_kernels = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "\n",
    "    # Assume your minority class is the class with fewer samples in the dataset\n",
    "    minority_class = y.value_counts().idxmin()\n",
    "\n",
    "    # txt_file = open(filename, \"w\")\n",
    "\n",
    "    # Start the evaluation for each kernel\n",
    "    for kernel in classical_kernels:\n",
    "        classical_svc = SVC(kernel=kernel)\n",
    "        classical_svc.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = classical_svc.predict(X_test)\n",
    "        classical_score = classical_svc.score(X_test, y_test)\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=[minority_class])\n",
    "\n",
    "        # The number of correctly predicted samples for the minority class\n",
    "        correct_minority_predictions = cm[0, 0]  # True Positives for the minority class\n",
    "\n",
    "        # Calculate the total number of minority class samples in the test set\n",
    "        total_minority_class_samples = (y_test == minority_class).sum()\n",
    "\n",
    "        # Calculate the percentage of correctly predicted minority class samples\n",
    "        minority_class_percentage = (\n",
    "            correct_minority_predictions / total_minority_class_samples\n",
    "        ) * 100\n",
    "\n",
    "        print(f\"{kernel} kernel classification test score:  {classical_score:.3f}\")\n",
    "        print(\n",
    "            f\"Correctly predicted samples for the minority class ({minority_class}): {correct_minority_predictions}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Percentage of correctly predicted minority class samples: {minority_class_percentage:.3f}%\"\n",
    "        )\n",
    "\n",
    "\n",
    "def quantum_ml(\n",
    "    feature_map, entanglement, X_train, y_train, X_test, y_test, y, file, eval_file\n",
    "):\n",
    "    # 使用する特徴量の数\n",
    "    objective_func_vals = []\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # ZZfeatureMapを作成\n",
    "    # feature_dimention: 特徴量数、reps: 繰り返し数\n",
    "    feature_map.decompose().draw(output=\"mpl\", fold=20)\n",
    "\n",
    "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3, entanglement=entanglement)\n",
    "\n",
    "    # 全ての量子ビットに対して測定を追加\n",
    "    ansatz.measure_all()\n",
    "\n",
    "    # 作成された回路を描写\n",
    "    ansatz.decompose().draw(output=\"mpl\", fold=20)\n",
    "\n",
    "    # 量子回路を作成\n",
    "    circuit = feature_map.compose(ansatz)\n",
    "\n",
    "    # 最終的な回路の描写\n",
    "    circuit.decompose().draw(output=\"mpl\")\n",
    "\n",
    "    # 学習経過を描写するコールバックを定義\n",
    "    def _callback_graph(weights, obj_func_eval):\n",
    "        objective_func_vals.append(obj_func_eval)\n",
    "\n",
    "        with open(file, \"a\") as f:\n",
    "            f.write(f\"{obj_func_eval}\\n\")\n",
    "\n",
    "    # construct variational quantum classifier\n",
    "    vqc = VQC(\n",
    "        feature_map=feature_map,  # 特徴量マップの指定\n",
    "        ansatz=ansatz,  # Ansatzの指定\n",
    "        loss=\"cross_entropy\",  # 訓練時に使用する損失関数\n",
    "        # optimizer=COBYLA(maxiter=30), # 訓練時に使用する最適化アルゴリズム\n",
    "        optimizer=SPSA(maxiter=200),\n",
    "        callback=_callback_graph,  # 訓練中の中間データへのアクセス方式。ここでは先ほど設定したコールバック関数を適用。\n",
    "        # sampler=sampler\n",
    "    )\n",
    "\n",
    "    vqc.fit(X_train, np.array(y_train).reshape(-1, 1))\n",
    "\n",
    "    score = vqc.score(X_test, np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "    y_pred = vqc.predict(X_test)\n",
    "\n",
    "    minority_class = y.value_counts().idxmin()\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[minority_class])\n",
    "\n",
    "    # The number of correctly predicted samples for the minority class\n",
    "    correct_minority_predictions = cm[0, 0]  # True Positives for the minority class\n",
    "\n",
    "    # Calculate the total number of minority class samples in the test set\n",
    "    total_minority_class_samples = (y_test == minority_class).sum()\n",
    "\n",
    "    # Calculate the percentage of correctly predicted minority class samples\n",
    "    minority_class_percentage = (\n",
    "        correct_minority_predictions / total_minority_class_samples\n",
    "    ) * 100\n",
    "\n",
    "    with open(eval_file, \"w\") as f:\n",
    "        f.write(f\"{score=}\\n\")\n",
    "        f.write(f\"{minority_class_percentage}\")\n",
    "\n",
    "    return score, minority_class_percentage\n",
    "\n",
    "\n",
    "def get_feature_map(name, num_features, reps):\n",
    "    if name == \"Z\":\n",
    "        return ZFeatureMap(feature_dimension=num_features, reps=reps)\n",
    "    elif name == \"ZZ\":\n",
    "        return ZZFeatureMap(feature_dimension=num_features, reps=reps)\n",
    "    elif name == \"Pauli\":\n",
    "        return PauliFeatureMap(feature_dimension=num_features, reps=reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./results/\"\n",
    "\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.mkdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_dict[\"KDD2014_donors_10feat_nomissing_normalised.csv\"]\n",
    "\n",
    "df = balance_classes(df)\n",
    "df = sample_df(df)\n",
    "\n",
    "X, X_pca, y = reduce_dimension(df) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, train_size=0.8, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "classical_ml(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    y,\n",
    "    f\"{BASE_DIR}/classical_eval.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "\n",
    "for _feat in [\"Z\", \"ZZ\", \"Pauli\"]:\n",
    "\n",
    "    if not os.path.exists(f\"{BASE_DIR}/{_feat}\"):\n",
    "        os.mkdir(f\"{BASE_DIR}/{_feat}\")\n",
    "\n",
    "    for _reps in [1, 2, 3, 4, 5]:\n",
    "        feature_map = get_feature_map(_feat, num_features, _reps)\n",
    "\n",
    "        for entanglement in [\"full\", \"linear\", \"circular\", \"reverse_linear\"]: \n",
    "            quantum_ml(\n",
    "                feature_map,\n",
    "                entanglement,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                y,\n",
    "                f\"{BASE_DIR}/{_feat}/obj_func_eval-{_reps}-{entanglement}.txt\",\n",
    "                f\"{BASE_DIR}/{_feat}/quantum_eval-{_reps}-{entanglement}.txt\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".qiskit-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
